ä½ çš„ç¨‹å¼æ¶æ§‹å·²ç¶“å¾ˆå®Œæ•´ï¼Œé‡å°å¾®èª¿å°‘é‡è³‡æ–™ã€LoRA è¨­å®šèˆ‡ `loss: nan` å•é¡Œçš„è™•ç†éƒ½å¾ˆç´°ç·»ã€‚ä¸éä½ é‡åˆ° `loss: nan` çš„æƒ…æ³ï¼Œé€™è£¡æœ‰å¹¾é»å¯é€²ä¸€æ­¥å„ªåŒ–èˆ‡æ’éŒ¯ï¼š

---

## âœ… æ ¸å¿ƒå•é¡Œæ’æŸ¥å»ºè­°

### ğŸ” 1. **`tokenizer.as_target_tokenizer()` åœ¨æ–°ç‰ˆ Transformers å·²æ£„ç”¨**

ä½ ç›®å‰é€™æ®µå¯èƒ½å°è‡´æ„æ–™å¤–çš„ token è™•ç†ï¼š

```python
with tokenizer.as_target_tokenizer():
    labels = tokenizer(...)
```

**å»ºè­°ï¼šç›´æ¥å»æ‰ï¼Œä½¿ç”¨ tokenizer è™•ç†è¼¸å‡ºå³å¯ã€‚**

```python
labels = tokenizer(
    outputs,
    max_length=128,
    padding="max_length",
    truncation=True
)["input_ids"]
```

---

### ğŸ” 2. **æª¢æŸ¥æ˜¯å¦æœ‰ã€Œå…¨ -100ã€çš„ label**

é€™æœƒé€ æˆ `loss` ç„¡æ³•è¨ˆç®—ï¼ˆç©ºçš„æœ‰æ•ˆ tokenï¼‰ã€‚ä½ å¯ä»¥åŠ ä¸Šä»¥ä¸‹ debugï¼š

```python
for label in labels:
    if all(l == -100 for l in label):
        print("Warning: label all -100!")
```

å¦‚æœä½ ç™¼ç¾æœ‰ `prompt` è·Ÿ `completion` å¤ªçŸ­ï¼Œå°è‡´ label å…¨éƒ¨æ˜¯ paddingï¼Œè«‹æ”¹çŸ­ `max_length` æˆ–åš input é•·åº¦æª¢æŸ¥ã€‚

---

### ğŸ” 3. **æ¨¡å‹é è¨­ç‚º float32ï¼Œä½† LoRA åˆå§‹åŒ–å®¹æ˜“ä¸ç©©å®šï¼ˆå°¤å…¶åœ¨ CPUï¼‰**

å»ºè­°ä½  **é–‹å•Ÿ gradient checkpointing**ï¼š

```python
model.gradient_checkpointing_enable()
```

---

### ğŸ” 4. **Tokené‡è¤‡éƒ¨åˆ†æ²’è™•ç†å¥½ï¼Œå°è‡´å­¸ä¸åˆ°æ±è¥¿**

ä½ ç›®å‰æ˜¯é€™æ¨£ tokenizeï¼š

```python
inputs = examples["prompt"]
outputs = examples["completion"]
```

ä½†åœ¨ LLM ä¸­ï¼Œæœ€å¥½æ˜¯æŠŠ input/output åˆä½µç‚ºå–®ä¸€åºåˆ—æ ¼å¼ï¼Œä¾‹å¦‚ï¼š

```
<|user|>: ä½ çš„ prompt
<|assistant|>: ä½ çš„ completion
```

ç„¶å¾Œåªåœ¨ `completion` çš„éƒ¨åˆ†è¨­ç‚º labelã€‚

---

## âœ… å»ºè­°æœ€ä½³æ ¼å¼ï¼ˆè¨“ç·´æ ¼å¼èˆ‡æ¨™è¨˜ï¼‰

```python
def tokenize_function(examples):
    prompts = examples["prompt"]
    completions = examples["completion"]
    inputs = [f"<|user|>: {q}\n<|assistant|>: {a}" for q, a in zip(prompts, completions)]

    model_inputs = tokenizer(
        inputs,
        max_length=256,
        padding="max_length",
        truncation=True
    )

    # åªåœ¨ <|assistant|> ä¹‹å¾Œéƒ¨åˆ†å»ºç«‹ labels
    labels = []
    for input_text in inputs:
        user_split = input_text.split("<|assistant|>: ")
        if len(user_split) != 2:
            raise ValueError("æ ¼å¼éŒ¯èª¤ï¼Œç¼ºå°‘ <|assistant|>: ")
        assistant_text = user_split[1]
        label_ids = tokenizer(
            assistant_text,
            max_length=256,
            padding="max_length",
            truncation=True
        )["input_ids"]
        label_ids = [(id if id != tokenizer.pad_token_id else -100) for id in label_ids]
        labels.append(label_ids)

    model_inputs["labels"] = labels
    return model_inputs
```

---

## âœ… å¾®èª¿æª¢æŸ¥é»æ•´ç†

| æª¢æŸ¥é …ç›®                    | å»ºè­°                                  |
| ----------------------- | ----------------------------------- |
| Learning rate           | `5e-6` âœ…                            |
| Batch size              | `1` âœ…                               |
| Epochs                  | `10+` âœ…                             |
| Gradient clipping       | `max_grad_norm=1.0` âœ…               |
| Loss mask               | ç¢ºä¿ `-100` æœ‰è¨­å®š                       |
| Label éç©º                | `label` ä¸­ä¸è¦å…¨æ˜¯ `-100`                |
| Tokenizer padding token | æœ‰è¨­ `pad_token` âœ…                    |
| LoRA target\_modules    | æœ‰è¨­ `q_proj`, `k_proj` ç­‰ âœ…           |
| Model mode              | `gradient_checkpointing_enable()` âœ… |

---

## ğŸ”§ Bonus: è¨“ç·´é€²åº¦ç›£æ§å°æŠ€å·§

å¯åœ¨ `Trainer` åŠ ä¸Š `callbacks` ä¾†ç›£æ§ loss æ¼”è®Šï¼ˆç”šè‡³æ—©åœï¼‰ï¼š

```python
from transformers import EarlyStoppingCallback

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)
```

---

